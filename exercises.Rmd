# Writing a BUGS model

Here's a basic generalized linear mixed model (a regression style model with random effects and non-normal likelihood). Your task is to write down the BUGS code corresponding to the following verbal description of the model.

Consider an experiment with 16 litters of rats in one group and 16 litters in another group. Let $i$ index the groups and $j$ index the litters. Now suppose that there are $n_{i,j}$ pups born in each litter and there is a survival probability specific to each litter. Use this information to define the likelihood component of the model.

Now let's consider 'borrowing strength' across the litters in each group. Basically this supposes that there are litter-specific influences on survival, but that all the litters in the group come from a common context. Let the survival probabilities for the litters in one group come from a beta distribution with unknown hyperparameters. Do the same for the second group (with different hyperparameters from the first group). This defines the latent process (random effects) component of the model.

Now put a non-informative prior distribution on the hyperparameters in the model.

# Running and MCMC in NIMBLE or JAGS

Define a model in NIMBLE using the BUGS code from the previous problem. Then run a basic MCMC for the model either in NIMBLE or JAGS. How does the MCMC look?

Here are data you can use.
```
consts <- list(G = 2,N = 16, n = matrix(c(13, 12, 12, 11, 9, 10, 
       9, 9, 8, 11, 8, 10, 13, 10, 12, 9, 10, 9, 10, 5, 9, 9, 13, 
       7, 5, 10, 7, 6, 10, 10, 10, 7), nrow = 2))
data = list(r = matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 10, 8, 9, 
     12, 9, 11, 8, 9, 8, 9, 4, 8, 7, 11, 4, 4, 5, 5, 3, 7, 3, 7, 0), 
     nrow = 2))
```

# Customizing an MCMC in NIMBLE

Now try other samplers for some of the parameters in the model from the previous problems.

  - Try slice samplers on each of the hyperparameters, `a[1],a[2],b[1],b[2]`. 
  - Try block samplers on `{a[1],b[1]}` and `{a[2],b[2]}`.

Do either of those seem to have improved the MCMC performance?

# Using MCMC (the pump example)

1) Compare the MCMC performance of using univariate slice samplers on ```alpha``` and ```beta``` to the default samplers and to the blocked Metropolis sampler for ```{alpha, beta}``` for longer MCMC runs. 

2) Change the sampler for ```theta``` to be a single block Metropolis sampler. How does that affect MCMC performance? How much time does it take to run a set of 10 individual conjugate samplers on the elements of ```theta``` (from the default configuration) compared to a single block sampler on the vector ```theta```. 

3) Suppose you wanted to consider the predictive distribution for a new pump. 
    - Create a new version of the model that allows for this.  
    - Set the data for the new model such that the new pump's number of failures is missing and not treated as a data node (see the data_nodes module for information on working with data).
    - Set up an MCMC for the new model. What kind of sampling should happen for the new pump failures? 

# Writing a model and operating it

1) Write BUGS code for a normal random effects model with 6 groups and 10 observations in each group.

2) Set the hyperparameters (the variance components) for the model and simulate the random means and observations. Now fix the data nodes as 'data'. See what happens if you try to use *simulate()* on the data nodes.

3) Calculate the full density of the model. Next calculate just the log-likelihood.

4) Compile the model and compare the time of calculating the full density in the compiled model to the original uncompiled R version of the model. (It may not be much different here for such a small model). You could use *system.time()* or for this comparison of a very fast calculation, *microbenchmark()* in the  *microbenchmark* package is likely to be more helpful.



# User-defined distributions

1) Simulate some data under the topic model given above and then fit the data via MCMC. Compare computational time and mixing for the multinomial likelihood plus Dirichlet prior model to those from the Dirichlet-multinomial representation.

2) Write the Pareto distribution as a user-defined distribution.

3) Write an "IID normal" distribution that can be used for a vector of $n$ normally distributed random variables, $y_i \sim N(\mu, \sigma)$. Compare the speed of an MCMC based on using a for loop and *dnorm* in BUGS code to that based on using the user-defined distribution. When using the standard specification, make sure the MCMC does not use conjugate (Gibbs) sampling so results are comparable (you can set `useConjugacy=FALSE` in *configureMCMC()*). This makes for an apples to apples comparison since our MCMC system won't know how to use your IID normal distribution for conjugate updates. 

# User-defined samplers

1) Write a user-defined sampler that modifies NIMBLE's default Metropolis (*sampler_RW()*) sampler to use a gamma proposal distribution and includes the ratio of the proposal distributions (the Hastings adjustment) for a non-symmetric proposal distribution. Have your proposal centered on the mean of the gamma distribution. When you call *rgamma* in the run function, you'll want to use the {mean, sd} alternative parameterization of the  gamma distribution.

2) Write a user-defined sampler that operates only on the categorical distribution. You could have it be an independence sampler with fixed probabilities of proposing each of the values or a sampler that puts a fixed amount of mass on the current value and distributes the remainder such that the probabilities decays as the difference from the current value increases. 


3) Consider exercise 2 and modify the probabilities such that they are proportional to the current density of the model for each possible value of the categorical variable. This is a conjugate sampler on a categorical variables. 


# Compiling R code

1) Let's consider using a nimbleFunction to replace a for loop that can't be avoided in R. Write a second order random walk using a nimbleFunction. Here's the code for the R version. 

```{r, markov-exer}
set.seed(0)
n <- 1e6
path <- rep(0, n)
rho1 <- .8
rho2 <- .1
path[1:2] <- rnorm(2)
print(system.time(
for(i in 3:n)
      path[i] <- rho1*path[i-1] + rho2*path[i-2] + rnorm(1)
))
tsplot(path[1:5000])
```

Now fill out the nimbleFunction version and test the timing.

```{r, markov-exer-scaffold, eval=FALSE}
mc <- nimbleFunction(
   run = function( ... ) ) {
       returnType( ... )
       ...
       return(...)
})
cmc <- compileNimble(mc)
set.seed(0)
system.time(path <- cmc(n, rho1, rho2))
```

2) Generalize your code to work for an arbitrary order of dependence.

3) Use *nimStop()* as part of an error check that ensures that the length of the path to be sampled is longer than the order of the dependence. 


# solution to problem 1

```{r, solution, eval=FALSE, echo=FALSE}
set.seed(0)
n <- 1e6
path <- rep(0, n)
rho1 <- .8
rho2 <- .1
path[1:2] <- rnorm(2)
print(system.time(
for(i in 3:n)
      path[i] <- rho1*path[i-1] + rho2*path[i-2] + rnorm(1)
))
tsplot(path[1:5000])


mc <- nimbleFunction(
   run = function(n = double(0), rho1 = double(0), rho2 = double(0)) {
       returnType(double(1))
       declare(path, double(1, n))
       path[1] <- rnorm(1)
       path[2] <- rnorm(1)
       for(i in 3:n) 
             path[i] <- rho1*path[i-1] + rho2*path[i-2] + rnorm(1)
       return(path)
})
cmc <- compileNimble(mc)
set.seed(0)
system.time(path <- cmc(n, rho1, rho2))
```

Not bad: going to C++ gives us a speedup of approximately 40-fold. 
